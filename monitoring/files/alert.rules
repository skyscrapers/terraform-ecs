groups:
- name: basic
  rules:

  # Alert for any instance that is unreachable for >5 minutes.
  - alert: service_down
    expr: 100  * (count(up == 0) BY (job) / count(up) BY (job)) > 10
    for: 2m
    labels:
      severity: page
    annotations:
      summary: "Instance {{ $labels.instance }} down"
      description: "{{ $labels.instance }} of job {{ $labels.job }} has been down for more than 2 minutes."

  - alert: high_load
    expr: node_load1 > 0.5
    for: 2m
    labels:
      severity: page
    annotations:
      summary: "Instance {{ $labels.instance }} under high load"
      description: "{{ $labels.instance }} of job {{ $labels.job }} is under high load."

  - alert: DeadMansSwitch
    expr: vector(1)
    labels:
      severity: none
    annotations:
      description: This is a DeadMansSwitch meant to ensure that the entire Alerting pipeline
        is functional.
      summary: Alerting DeadMansSwitch
  - alert: NodeExporterDown
    expr: absent(up{job="node-exporter"} == 1)
    for: 10m
    labels:
      severity: warning
    annotations:
      description: Prometheus could not scrape a node-exporter for more than 10m, or node-exporters
        have disappeared from discovery
      summary: Prometheus could not scrape a node-exporter
  - alert: NodeDiskRunningFull
    expr: predict_linear(node_filesystem_free[30m], 3600 * 2) < 0
    for: 10m
    labels:
      severity: critical
    annotations:
      description: device {{$labels.device}} on node {{$labels.instance}} is running full
        within the next 2 hours (mounted at {{$labels.mountpoint}})
      summary: Node disk is running full
